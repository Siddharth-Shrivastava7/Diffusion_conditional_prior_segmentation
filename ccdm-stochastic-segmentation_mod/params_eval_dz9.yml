output_path: "/home/sidd_s/scratch/ccdm_logs/darkzurich_majority_single_conditionalposterior_bt1_no_image_condition_2samples"
evaluation_path: "./evaluation/results"
evaluations: 8  # for multiple evaluations with majority vote and for LIDC number of samples
evaluation_vote_strategy: "majority"  # One of ["majority", "confidence"]  

dataset_file: datasets.darkzurich
dataset_split: 'val'  # One of ['val', 'test']
dataset_val_max_size: 2 # One of "null" (=full val size), or any number
class_weights: "uniform"  # One of ["uniform", "weighted"]
save_qualitative_results:
    enable: yes
    num_images: 3  # will be doubled for random imgs
    num_predictions: 3

dataset_pipeline_train: ["flip",  "resize", "colorjitter", "torchvision_normalise"]
dataset_pipeline_train_settings:
    target_size:  [256, 512]

dataset_pipeline_val: ["resize", "torchvision_normalise"]
dataset_pipeline_val_settings:
    target_size: [256, 512]
    return_original_labels: yes

evaluation:
    resolution: "dataloader" # One of ["dataloader", "original"] # using "dataloader" is better if want to increase speed and should use "confidence" if want to have "original" resolution via bilinear upsampling
    evaluations: 10  # for multiple evaluations with confidence vote
    evaluation_vote_strategy: "confidence"  # One of ["majority", "confidence"]

multigpu: no
distributed: no
mp_loaders: 0
batch_size: 1 # why here batch size coming into picture 
max_epochs: 7500
wandb: no

polyak_alpha: 0.999 
beta_schedule: "cosine" # One of ["cosine", "linear"]
beta_schedule_params:
    s: 0.008
    # start: 0.01
    # end: 0.1 ## this (st: 0.01, end: 0.0008) is falling back to MIC model output that means going back to x_T which is not desired thus, beta should always be increasing s.t. start < end
time_steps: 250 # changing number of time steps as starting from close outputs

backbone: "unet_openai"  # One of ["unetplus", "unet_openai", "resnet50", "resnet101"]

feature_cond_encoder:
    type: 'dino' # ['dino', 'none']
    model: 'dino_vits8' # 'dino_vitb8
    channels: 384 # 768 for vitb, 384 for vits
    conditioning: "concat_pixels_concat_features"
    output_stride: 8  # at what output_stride in the unet to plug features
    scale: 'single'
    train: no
    source_layer: 11 # layer 11 is always the last layer of any dino vit model
    target_layer: 10 # at what output_stride in the unet to plug features


unet_openai:
    base_channels: 32
    channel_mult: null  # 9M: [1, 2, 2, 4, 5]
    attention_resolutions: [32, 16, 8]
    num_heads: 1  # Ignored if num_head_channels is not -1
    num_head_channels: 32  # If not -1, num_heads is automatically set to channels//num_head_channels
    softmax_output: yes # this is the default for build_model
    ce_head: no # adds an extra head that predicts logits (distinct from denoising head)

load_from: '/home/sidd_s/scratch/ccdm_logs/output_cityscapes_256x512_bt16/best_checkpoint_122500_mIoU=0.5293.pt'


## two additional conditional checking if needed or not
check_feature_condition: True
check_image_condition: False
Metadata-Version: 2.1
Name: mmsegmentation
Version: 0.29.0
Summary: Open MMLab Semantic Segmentation Toolbox and Benchmark
Home-page: http://github.com/open-mmlab/mmsegmentation
Author: MMSegmentation Contributors
Author-email: openmmlab@gmail.com
License: Apache License 2.0
Keywords: computer vision,semantic segmentation
Classifier: Development Status :: 4 - Beta
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSES.md
Requires-Dist: matplotlib
Requires-Dist: mmcls>=0.20.1
Requires-Dist: numpy
Requires-Dist: packaging
Requires-Dist: prettytable
Requires-Dist: einops
Requires-Dist: tqdm
Provides-Extra: all
Requires-Dist: cityscapesscripts; extra == "all"
Requires-Dist: matplotlib; extra == "all"
Requires-Dist: mmcls>=0.20.1; extra == "all"
Requires-Dist: numpy; extra == "all"
Requires-Dist: packaging; extra == "all"
Requires-Dist: prettytable; extra == "all"
Requires-Dist: einops; extra == "all"
Requires-Dist: tqdm; extra == "all"
Requires-Dist: codecov; extra == "all"
Requires-Dist: flake8; extra == "all"
Requires-Dist: interrogate; extra == "all"
Requires-Dist: pytest; extra == "all"
Requires-Dist: xdoctest>=0.10.0; extra == "all"
Requires-Dist: yapf; extra == "all"
Provides-Extra: tests
Requires-Dist: codecov; extra == "tests"
Requires-Dist: flake8; extra == "tests"
Requires-Dist: interrogate; extra == "tests"
Requires-Dist: pytest; extra == "tests"
Requires-Dist: xdoctest>=0.10.0; extra == "tests"
Requires-Dist: yapf; extra == "tests"
Provides-Extra: build
Provides-Extra: optional
Requires-Dist: cityscapesscripts; extra == "optional"
Provides-Extra: mim
Requires-Dist: mmcls>=0.20.1; extra == "mim"
Requires-Dist: mmcv-full<1.7.0,>=1.4.4; extra == "mim"

# Applying DDP for Semantic Segmentation

Our segmentation code is developed on top of MMSegmentation v0.20.2.

For details please see [DDP](https://arxiv.org/abs/2303.17559)

---

If this work is helpful for your research, please consider citing the following BibTeX entry.
```
@article{ji2023ddp,
  title={DDP: Diffusion Model for Dense Visual Prediction},
  author={Ji, Yuanfeng and Chen, Zhe and Xie, Enze and Hong, Lanqing and Liu, Xihui and Liu, Zhaoqiang and Lu, Tong and Li, Zhenguo and Luo, Ping},
  journal={arXiv preprint arXiv:2303.17559},
  year={2023}
}
```

## Installation
The code is based on the MMSegmentation v0.29.0+

```
# recommended to create a new environment with torch1.12 + cuda11.6
pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 -f https://download.pytorch.org/whl/torch_stable.html
pip install mmcv-full==1.6.2 -f https://download.openmmlab.com/mmcv/dist/cu116/torch1.12.0/index.html
cd DDP/segmentation/
pip install -v -e .
```
## Data Preparation

Preparing ADE20K/Cityscapes according to the [guidelines](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/dataset_prepare.md#prepare-datasets) in MMSegmentation.

## Results and Models

**Cityscapes val (step 3)**

|  Backbone  | Lr schd | Crop Size | mIoU (SS/MS) | #Param |                                   Config                                    |                                                                                                        Download                                                                                                        |
|:----------:|:-------:|:---------:|:------------:|:------:|:---------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| ConvNext-T |  160K   | 512x1024  | 82.60/83.15  |  40M   | [config](configs/cityscapes/ddp_convnext_t_4x4_512x1024_160k_cityscapes.py) | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_t_4x4_512x1024_160k_cityscapes.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_t_4x4_512x1024_160k_cityscapes.log) |
| ConvNext-S |  160K   | 512x1024  | 82.69/83.58  |  62M   |    [config](configs/cityscapes/ddp_convnext_s_4x4_512x1024_160k_cityscapes.py)     | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_b_4x4_512x1024_160k_cityscapes.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_s_4x4_512x1024_160k_cityscapes.log) |
| ConvNext-B |  160K   | 512x1024  | 82.78/83.49  |  100M  |    [config](configs/cityscapes/ddp_convnext_b_4x4_512x1024_160k_cityscapes.py)     | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_s_4x4_512x1024_160k_cityscapes.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_b_4x4_512x1024_160k_cityscapes.log) |
| ConvNext-L |  160K   | 512x1024  | 83.21/83.92  |  209M  |    [config](configs/cityscapes/ddp_convnext_l_4x4_512x1024_160k_cityscapes.py)     | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_l_4x4_512x1024_160k_cityscapes.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_convnext_l_4x4_512x1024_160k_cityscapes.log) |

[//]: # (|   Swin-T   |  160K   | 512x1024  | 81.24/82.46  |  39M   |   [config]&#40;./configs/ddp/ddp_swin_t_4x4_512x1024_160k_cityscapes.py&#41;   | [ckpt]&#40;&#41; \ [log]&#40;&#41; |)

[//]: # (|   Swin-S   |  160K   | 512x1024  | 82.41/83.21  |  61M   |   [config]&#40;./configs/ddp/ddp_swin_s_4x4_512x1024_160k_cityscapes.py&#41;   | [ckpt]&#40;&#41; \ [log]&#40;&#41; |)

[//]: # (|   Swin-B   |  160K   | 512x1024  | 82.54/83.42  |  99M   |   [config]&#40;./configs/ddp/ddp_swin_b_4x4_512x1024_160k_cityscapes.py&#41;   | [ckpt]&#40;&#41; \ [log]&#40;&#41; |)

**ADE20k val (step 3)**

| Backbone | Lr schd | Crop Size | mIoU (SS/MS) | #Param |                            Config                             |                                                                                               Download                                                                                               |
|:--------:|:-------:|:---------:|:------------:|:------:|:-------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|  Swin-T  |  160K   |  512x512  |  47.0/47.8   |  40M   | [config](configs/ade/ddp_swin_t_2x8_512x512_160k_ade20k.py) | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_swin_t_2x8_512x512_160k_ade20k.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_swin_t_2x8_512x512_160k_ade20k.log) |
|  Swin-S  |  160K   |  512x512  |  48.7/49.7   |  61M   |                              TBA                              |                                                                                              TBA \ TBA                                                                                               |
|  Swin-B  |  160K   |  512x512  |  49.4/50.8   |  99M   |                              TBA                              |                                                                                              TBA \ TBA                                                                                               |
|  Swin-L  |  160K   |  512x512  |  53.2/54.4   |  207M  | [config](configs/ade/ddp_swin_l_2x8_512x512_160k_ade20k.py) | [ckpt](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_swin_l_2x8_512x512_160k_ade20k.pth) \ [log](https://huggingface.co/datasets/yfji/DDP/blob/main/ddp_swin_l_2x8_512x512_160k_ade20k.log) |

## Training

multi-gpu training
```
bash tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM}
```
For example, To train DDP-ConvNext-L on cityscapes with 4 gpus run:
```
bash tools/dist_train.sh configs/cityscapes/ddp_convnext_l_4x4_512x1024_160k_cityscapes.py 4
```

## Evaluation

single-gpu testing
```
python tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE} --eval mIoU
```

multi-gpu testing
```
bash tools/dist_test.sh ${CONFIG_FILE} ${CHECKPOINT_FILE} ${GPU_NUM} --eval mIoU
```

For example, To evaluate DDP-ConvNext-T on cityscapes val on a single node with 4 gpus run:
```
bash tools/dist_test.sh configs/cityscapes/ddp_convnext_t_4x4_512x1024_160k_cityscapes.py ckpts/ddp_convnext_t_4x4_512x1024_160k_cityscapes.pth 4 --eval mIoU
```
This should give the below results. Note that the results may vary a little on different machines due to the randomness of the diffusion modeling.
```
Summary:

+-------+-------+-------+
|  aAcc |  mIoU |  mAcc |
+-------+-------+-------+
| 96.85 | 82.71 | 88.76 |
+-------+-------+-------+
```

## Image Demo

To inference a single image like this:
```
python image_demo.py ${IMAGE_FILE} ${CONFIG_FILE} ${CHECKPOINT_FILE} ${PRED_FILE} --device cuda:0 --palette ${PALETTE_FILE}
```
For example, the result will be saved in `resources/demo_pred.png` by running:
```
python image_demo.py resources/demo.png configs/ddp/ddp_convnext_t_4x4_512x1024_160k_cityscapes.py ckpts/ddp_convnext_t_4x4_512x1024_160k_cityscapes.pth resources/demo_pred.png --device cuda:0 --palette cityscapes
```




